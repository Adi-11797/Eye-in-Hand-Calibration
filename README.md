# Eye-in-Hand-Calibration
Eye-in-Hand Calibration of a manipulator robot arm using SLAM techniques

In this project, a self-calibration approach for eye-in hand robots using SLAM is considered. The goal is to calibrate the positioning of a robotic arm, with a camera mounted on the end-effector automatically using a SLAM-based method like Extended Kalman Filter (EKF). Given the camera intrinsic parameters and a set of feature markers in a work-space, the camera extrinsic parameters are approximated. An EKF based measurement model is deployed to effectively localize the camera and compute the camera to end-effector transformation. The proposed approach is tested on a UR5 manipulator with a depth camera mounted on the end-effector to validate our results.

Our approach for eye-in-hand calibration using SLAM, can be broadly broken down into the steps shown in Fig. 2. We use an Extended Kalman Filter (EKF) based approach to localize the camera on the end-effector, which is predicted based on the control inputs to the end-effector and updated with our measurement data. For simplification, here the control inputs to the end-effector are omitted and we perform the localization with a pre-defined trajectory. The trajectory is defined by moving the end-effector to different locations in the 3D space, ensuring ample variations (atleast 20-40 mm) in the x, y and z coordinates of the base frame and all markers are visible in the field of view.. We employ feature markers for gathering the measurements using key point detection. The primary purpose of these markers are to pass relevant information such as depth data between the camera and target coordinate frames and additionally to allow easy landmark association. Use of such feature markers is a common practice in robotic-arm calibration.

![slam1](https://user-images.githubusercontent.com/92863991/212874861-dc694906-dc41-43db-a32b-fb8778edbee9.png)

Using a camera projection model the feature markers are scanned online, and the center of the feature markers are mapped to the camera coordinate system. These projected coordinates are subsequently used as landmark poses in the state vector of our localization algorithm, while the depth information from our sensor will provide the respective measurement data. 

Finally, EKF based camera localization provides our estimate of the camera pose within the global frame which is used to infer the initial transformation matrix between the camera and end-effector positions. This matrix is iteratively updated as we arrive at new waypoints in the trajectory and as the sensor revisits previously mapped landmarks, the accuracy of our model incrementally improves.

![sl2](https://user-images.githubusercontent.com/92863991/212874860-40e620c9-a2be-4a63-8f8f-67721a93597e.png)



<ins>Experimental Setup</ins>:

Our hardware test setup includes three components; an in-house Universal Robots (UR5) robotic manipulator arm with 6 degrees of freedom, an Intel real-sense D-435i depth camera, and a set of eight Quick Response (QR) markers which behave as static landmarks. The UR5 manipulator is mounted onto the wall, while the camera is seen mounted perpendicularly on the end-effector. We define the positive Z-axis within the base frame and the end-effector frame as projecting from the wall towards the user, the Y-axis as a vertical-upward traversal (downward for negative Y), and the X-axis as horizontal-right traversal (left for negative X). The camera frame is oriented differently from the base frame, as can be seen in Fig. 6.

The camera is placed at a predetermined location on the robot end-effector approximately 6mm along the positive Y-axis and 40 mm along the positive Z-axis from the end-effector frame. The camera provides the image data for the key-point detection and feature extraction to run the localization and calibration algorithms, while the QR markers are placed in a planar fashion within the X-Z plane of base frame.

![sl3](https://user-images.githubusercontent.com/92863991/212874859-e1859b4e-e101-4e75-9f89-293a1b467cc8.png)

The dataset used to test our algorithm is generated by traversing through a set of predetermined way-points within the 3D robot workspace. These way-points are provided via a teach pendant and indicates the position of the end-effector tool-centre with respect to the robot base-frame.

![1](https://user-images.githubusercontent.com/92863991/212873345-50953a35-31e9-402b-b445-c5572354d226.png)



<ins>Results</ins>:

Using the data collected from the experimental setup, we used our EKF SLAM algorithm to localize the camera and then perform calibration. From the EKF algorithm, we finally get the location of the camera in the base frame, is assumed to be at the base of the robot. The localization results can be seen below.

![sl4](https://user-images.githubusercontent.com/92863991/212874855-9168c7ce-9129-406a-9559-22ce920206c9.png)

